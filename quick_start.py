"""
å¿«é€Ÿå…¥é—¨è„šæœ¬ - å¤šä»»åŠ¡LSTMæ¨¡å‹æµ‹è¯•

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. è‡ªåŠ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®
2. è¿è¡Œä¸€ä¸ªå°è§„æ¨¡çš„è®­ç»ƒæµ‹è¯•ï¼ˆ2ä¸ªepochï¼‰
3. éªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ

é€‚åˆç”¨äºï¼š
- é¦–æ¬¡è¿è¡Œå‰çš„ç¯å¢ƒæµ‹è¯•
- éªŒè¯ä»£ç æ˜¯å¦æ­£å¸¸å·¥ä½œ
- ç†è§£æ•´ä¸ªæµç¨‹
"""

import os
import sys
from pathlib import Path

print("=" * 60)
print("å¤šä»»åŠ¡LSTMæ¨¡å‹ - å¿«é€Ÿå…¥é—¨æµ‹è¯•")
print("=" * 60)

# æ£€æŸ¥ä¾èµ–åŒ…
print("\næ­¥éª¤ 1/5: æ£€æŸ¥ä¾èµ–åŒ…...")
required_packages = [
    'numpy', 'pandas', 'xarray', 'torch', 
    'tqdm', 'matplotlib', 'hydrodataset', 'HydroErr'
]

missing_packages = []
for package in required_packages:
    try:
        __import__(package if package != 'HydroErr' else 'HydroErr')
        print(f"  âœ“ {package}")
    except ImportError:
        print(f"  âœ— {package} (ç¼ºå¤±)")
        missing_packages.append(package)

if missing_packages:
    print(f"\né”™è¯¯ï¼šç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…ï¼š{', '.join(missing_packages)}")
    print(f"è¯·è¿è¡Œ: pip install {' '.join(missing_packages)}")
    sys.exit(1)

print("  æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…ï¼")

# æ£€æŸ¥CAMELSæ•°æ®
print("\næ­¥éª¤ 2/5: æ£€æŸ¥CAMELSæ•°æ®...")
camels_path = Path("camels/camels_us")
if not camels_path.exists():
    print(f"  âœ— CAMELSæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {camels_path}")
    print(f"  è¯·ä¸‹è½½CAMELSæ•°æ®é›†å¹¶æ”¾ç½®åœ¨æ­£ç¡®ä½ç½®")
    sys.exit(1)

try:
    from hydrodataset.camelsh import Camelsh
    # æŒ‡å®šCAMELSHæ•°æ®è·¯å¾„
    camelsh_data_path = "camelsh_data"  # ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ•°æ®è·¯å¾„
    camelsh = Camelsh(camelsh_data_path, download=False)
    print(f"  âœ“ CAMELSHæ•°æ®åŠ è½½æˆåŠŸ")
    print(f"  æ•°æ®è·¯å¾„: {camelsh.data_source_dir}")
    
    # è·å–æµåŸŸåˆ—è¡¨
    basin_ids = camelsh.read_object_ids()
    basins = basin_ids[:2].tolist()
    print(f"  æµ‹è¯•æµåŸŸ: {basins}")
except Exception as e:
    print(f"  âœ— CAMELSæ•°æ®åŠ è½½å¤±è´¥: {e}")
    sys.exit(1)

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
print("\næ­¥éª¤ 3/5: ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
try:
    from create_sample_data import generate_sample_data
    
    generate_sample_data(
        basins=basins,
        start_date="1990-09-01",
        end_date="2010-08-31",
        output_flow="flow_data.csv",
        output_waterlevel="waterlevel_data.csv"
    )
    print("  âœ“ ç¤ºä¾‹æ•°æ®ç”ŸæˆæˆåŠŸ")
except Exception as e:
    print(f"  âœ— ç¤ºä¾‹æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
    sys.exit(1)

# è¿è¡Œå¿«é€Ÿæµ‹è¯•
print("\næ­¥éª¤ 4/5: è¿è¡Œæ¨¡å‹è®­ç»ƒæµ‹è¯•...")
print("  (ä½¿ç”¨å°è§„æ¨¡å‚æ•°è¿›è¡Œå¿«é€Ÿæµ‹è¯•)")

try:
    import random
    import numpy as np
    import pandas as pd
    import xarray as xr
    import torch
    from torch.utils.data import DataLoader
    import torch.nn as nn
    from tqdm import tqdm
    
    # å¯¼å…¥æ¨¡å‹ç±»
    from multi_task_lstm import (
        MultiTaskDataset, 
        MultiTaskLSTM, 
        train_epoch, 
        eval_model,
        set_random_seed,
        DEVICE
    )
    import HydroErr as he
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed(1234)
    
    # å¯¼å…¥æ ‡å‡†å˜é‡
    from hydrodataset import StandardVariable
    
    # é€‰æ‹©å˜é‡
    chosen_forcing_vars = [
        StandardVariable.PRECIPITATION,
        StandardVariable.TEMPERATURE_MEAN,
        StandardVariable.SOLAR_RADIATION,
        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION
    ]
    chosen_attrs_vars = [
        "p_mean", "p_seasonality", "frac_snow", "aridity", "area"
    ]
    
    # åŠ è½½å±æ€§æ•°æ®
    attrs = camelsh.read_attr_xrdataset(
        gage_id_lst=basins,
        var_lst=chosen_attrs_vars
    )
    print(f"  å±æ€§æ•°æ®å½¢çŠ¶: {attrs.dims}")
    
    # å‡†å¤‡æ°”è±¡æ•°æ®ï¼ˆä½¿ç”¨è¾ƒçŸ­çš„æ—¶é—´èŒƒå›´è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼‰
    train_times = ["2010-01-01", "2012-12-31"]
    valid_times = ["2013-01-01", "2014-12-31"]
    
    # ä½¿ç”¨CAMELSHæ¥å£åŠ è½½æ°”è±¡æ•°æ®
    train_forcings = camelsh.read_ts_xrdataset(
        gage_id_lst=basins,
        t_range=train_times,
        var_lst=chosen_forcing_vars
    )
    valid_forcings = camelsh.read_ts_xrdataset(
        gage_id_lst=basins,
        t_range=valid_times,
        var_lst=chosen_forcing_vars
    )
    
    # åŠ è½½å¾„æµå’Œæ°´ä½æ•°æ®
    train_flow_ds = camelsh.read_ts_xrdataset(
        gage_id_lst=basins,
        t_range=train_times,
        var_lst=[StandardVariable.STREAMFLOW]
    )
    train_waterlevel_ds = camelsh.read_ts_xrdataset(
        gage_id_lst=basins,
        t_range=train_times,
        var_lst=[StandardVariable.WATER_LEVEL]
    )
    valid_flow_ds = camelsh.read_ts_xrdataset(
        gage_id_lst=basins,
        t_range=valid_times,
        var_lst=[StandardVariable.STREAMFLOW]
    )
    valid_waterlevel_ds = camelsh.read_ts_xrdataset(
        gage_id_lst=basins,
        t_range=valid_times,
        var_lst=[StandardVariable.WATER_LEVEL]
    )
    
    # è½¬æ¢ä¸ºpandas DataFrameæ ¼å¼
    train_flow = train_flow_ds[StandardVariable.STREAMFLOW].to_pandas().T
    train_waterlevel = train_waterlevel_ds[StandardVariable.WATER_LEVEL].to_pandas().T
    valid_flow = valid_flow_ds[StandardVariable.STREAMFLOW].to_pandas().T
    valid_waterlevel = valid_waterlevel_ds[StandardVariable.WATER_LEVEL].to_pandas().T
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨å°å‚æ•°ï¼‰
    sequence_length = 50  # è¾ƒçŸ­çš„åºåˆ—
    batch_size = 16       # è¾ƒå°çš„æ‰¹æ¬¡
    
    ds_train = MultiTaskDataset(
        basins=basins,
        dates=train_times,
        data_attr=chosen_attrs,
        data_forcing=train_forcings,
        data_flow=train_flow,
        data_waterlevel=train_waterlevel,
        loader_type="train",
        seq_length=sequence_length,
    )
    tr_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)
    
    means = ds_train.get_means()
    stds = ds_train.get_stds()
    
    ds_val = MultiTaskDataset(
        basins=basins,
        dates=valid_times,
        data_attr=chosen_attrs,
        data_forcing=valid_forcings,
        data_flow=valid_flow,
        data_waterlevel=valid_waterlevel,
        loader_type="valid",
        seq_length=sequence_length,
        means=means,
        stds=stds,
    )
    val_loader = DataLoader(ds_val, batch_size=100, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨å°å‚æ•°ï¼‰
    input_size = len(chosen_attrs_vars) + len(chosen_forcing_vars)
    hidden_size = 32      # è¾ƒå°çš„éšè—å±‚
    dropout_rate = 0.1
    learning_rate = 1e-3
    
    task_weights = {'flow': 1.0, 'waterlevel': 1.0}
    
    model = MultiTaskLSTM(
        input_size=input_size,
        hidden_size=hidden_size,
        dropout_rate=dropout_rate,
        task_weights=task_weights
    ).to(DEVICE)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    loss_func = nn.MSELoss()
    
    print(f"  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(ds_train)}")
    print(f"  éªŒè¯æ ·æœ¬æ•°: {len(ds_val)}")
    
    # å¿«é€Ÿè®­ç»ƒï¼ˆä»…2ä¸ªepochï¼‰
    n_epochs = 2
    print(f"\n  å¼€å§‹è®­ç»ƒ ({n_epochs} epochs)...")
    
    for i in range(n_epochs):
        train_loss, _, _ = train_epoch(model, optimizer, tr_loader, loss_func, i + 1)
        
        # éªŒè¯
        obs_flow, obs_waterlevel, preds_flow, preds_waterlevel = eval_model(model, val_loader)
        
        preds_flow = ds_val.local_denormalization(preds_flow.cpu().numpy(), variable="flow")
        preds_waterlevel = ds_val.local_denormalization(preds_waterlevel.cpu().numpy(), variable="waterlevel")
        
        obs_flow = obs_flow.numpy().reshape(len(basins), -1)
        obs_waterlevel = obs_waterlevel.numpy().reshape(len(basins), -1)
        preds_flow = preds_flow.reshape(len(basins), -1)
        preds_waterlevel = preds_waterlevel.reshape(len(basins), -1)
        
        nse_flow = np.array([he.nse(preds_flow[j], obs_flow[j]) for j in range(len(basins))])
        nse_waterlevel = np.array([he.nse(preds_waterlevel[j], obs_waterlevel[j]) for j in range(len(basins))])
        
        print(f"  Epoch {i+1}: NSE(å¾„æµ)={nse_flow.mean():.4f}, NSE(æ°´ä½)={nse_waterlevel.mean():.4f}")
    
    print("\n  âœ“ æ¨¡å‹è®­ç»ƒæµ‹è¯•æˆåŠŸï¼")
    
except Exception as e:
    print(f"\n  âœ— æ¨¡å‹è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æ€»ç»“
print("\næ­¥éª¤ 5/5: æµ‹è¯•å®Œæˆæ€»ç»“")
print("=" * 60)
print("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
print("\nç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œä½ å¯ä»¥ï¼š")
print("  1. å‡†å¤‡ä½ çš„çœŸå®å¾„æµå’Œæ°´ä½æ•°æ®")
print("     - å‚è€ƒ flow_data.csv å’Œ waterlevel_data.csv çš„æ ¼å¼")
print("  2. è¿è¡Œå®Œæ•´è®­ç»ƒï¼špython multi_task_lstm.py")
print("  3. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼šMULTI_TASK_README.md")
print("\næç¤ºï¼š")
print("  - ç¤ºä¾‹æ•°æ®ä»…ç”¨äºæµ‹è¯•ï¼Œè¯·ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œå®é™…è®­ç»ƒ")
print("  - å®Œæ•´è®­ç»ƒå»ºè®®ä½¿ç”¨æ›´å¤§çš„hidden_sizeå’Œæ›´å¤šepochs")
print("=" * 60)
print("\nç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰")


